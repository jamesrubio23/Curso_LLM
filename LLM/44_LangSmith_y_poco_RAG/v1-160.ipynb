{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d15a62-599d-4c8f-9821-79f07ec73ff6",
   "metadata": {},
   "source": [
    "# RAG Phases: LangChain Quickstart\n",
    "* We will build a simple RAG app over a text data source.\n",
    "* We will see how LangSmith can help us trace and understand the RAG app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db563b3b-3b9b-46ce-815e-617e77d7cccd",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "Steps of the Indexing phase:\n",
    "* Load text with a [Document Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/).\n",
    "* Split text into small chunks with a [Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/).\n",
    "* Convert chunks into [embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/) and store them in a [vector database, also called vector store](https://python.langchain.com/docs/modules/data_connection/vectorstores/).\n",
    "\n",
    "Steps of the Retrieval and Generation phase:\n",
    "* Given a question, the most relevant chunks are retrieved from the vector database using a [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/).\n",
    "* The question and the chunks are sent to the [LLM](https://python.langchain.com/docs/modules/model_io/llms/) in a prompt. The LLM then produces the answer. You can use a [ChatModel](https://python.langchain.com/docs/modules/model_io/chat) instead of an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59939c81-6b6b-4987-84ed-8ffebc495501",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d89350a-9cfb-4fd4-a15c-8adfe688e234",
   "metadata": {},
   "source": [
    "#### Recommended: create new virtualenv\n",
    "* pyenv virtualenv 3.11.4 your_venv_name\n",
    "* pyenv activate your_venv_name\n",
    "* pip install jupyterlab\n",
    "* jupyter lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac6049-8508-47fc-bd49-ddf7e44d5e02",
   "metadata": {},
   "source": [
    "#### Dependencies\n",
    "* OpenAI Chat Model.\n",
    "* OpenAI Embeddings.\n",
    "* Chroma Vector Database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df588d-d5b2-404f-8f6b-2b64e67ea00e",
   "metadata": {},
   "source": [
    "#### Necessary Modules:\n",
    "* langchain\n",
    "* langchain-community\n",
    "* langchainhub\n",
    "* langchain-openai\n",
    "* chromadb\n",
    "* bs4: for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e9124f1-2235-4503-a8d9-57a1038f42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb367958-c00f-496a-8ebd-772a56d731a1",
   "metadata": {},
   "source": [
    "NOTE: In a Jupyter Notebook, both `%pip` and `!pip` are used to manage Python packages, but they operate slightly differently due to the way Jupyter interprets these commands.\n",
    "\n",
    "1. **`%pip` Command**: This is a Jupyter-specific magic command. When you use `%pip`, it ensures that the package installation or any other pip operation you are performing happens in the current kernel's environment. This is important in Jupyter because notebooks can be connected to different Python environments (kernels). The `%pip` magic command helps to avoid confusion about where packages are being installed, especially in cases where you might have multiple environments or kernels. It was introduced to address the common issue where users would install a package using `!pip` and then discover it was not available in the notebook because it was installed in a different Python environment.\n",
    "\n",
    "2. **`!pip` Command**: This command invokes the system shell and runs `pip`, Python's package installer, as if you were running it in your terminal or command prompt. While this does effectively install or upgrade packages, it may not always install them into the kernel that the Jupyter Notebook is currently using. This is because `!pip` executes pip in the environment from which Jupyter was started, which might be different from the current notebook's kernel environment. Before the introduction of `%pip`, this could lead to confusion among users about why installed packages were not available to their notebooks.\n",
    "\n",
    "In summary, while both commands can be used to install Python packages in Jupyter Notebooks, `%pip` is more Jupyter-aware and ensures that packages are installed in the environment associated with the current notebook's kernel, thus avoiding common environment mismatch issues. It is generally recommended to use `%pip` when working in Jupyter Notebooks to ensure consistency and avoid potential confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc3f93a-0d31-4b28-93c1-6bcc3fa0369e",
   "metadata": {},
   "source": [
    "#### .env File\n",
    "Remember to include:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa55c18-aa0a-4ec7-a54d-f7c91d46a259",
   "metadata": {},
   "source": [
    "* our LangSmith project name: RAGquickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c1696-09b0-4317-9856-5362bf4fdba0",
   "metadata": {},
   "source": [
    "#### Recommended: activate LangSmith from the beginning\n",
    "* LangSmith credentials in the .env file:\n",
    "    * LANGCHAIN_TRACING_V2=true\n",
    "    * LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "    * LANGCHAIN_API_KEY=your_key\n",
    "    * LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e74a9b-f3d9-49f0-a6e9-89a3aa1de0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de808b-c0e9-48a9-b334-666cf8482df8",
   "metadata": {},
   "source": [
    "## Open LangSmith to track the following operations\n",
    "* smith.langchain.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f8d23-2438-44dd-8347-a90323ad8344",
   "metadata": {},
   "source": [
    "## Goal of our RAG App\n",
    "* RAG App over the blog post [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) by Lilian Weng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008a3e7-4a6e-4ba0-825c-d849307b780d",
   "metadata": {},
   "source": [
    "#### Required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0468b4b2-8614-40c6-a3fb-82d594e10c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e6586-40f1-4313-8494-08155187e896",
   "metadata": {},
   "source": [
    "For demo purposes, we will import these modules again in the next cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557d37e-63c5-4801-8af8-03811d776bde",
   "metadata": {},
   "source": [
    "# Phase 1: Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af372dcd-6922-4d1e-a68a-44c20dc11b94",
   "metadata": {},
   "source": [
    "## Step #1 of the Indexing phase: Load data\n",
    "* WebBaseLoader loads HTML from web URLs.\n",
    "* BS4 selects what HTML classes and parses them into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830b9e9c-524e-4672-9625-874f780caa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d9fdc0-2c22-416d-8892-96e029220c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42823"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51663b94-5591-4e96-a403-a7a9869bd908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb09c4-144c-483a-b6ea-4fe644377f0c",
   "metadata": {},
   "source": [
    "#### Additional info on Loading Data\n",
    "* [Documentation on Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/). Interesting points:\n",
    "    * PDF. Check the Multimodal LLM Apps section.\n",
    "    * CSV.\n",
    "    * JSON.\n",
    "    * HTML.\n",
    "* [Integrations with 3rd Party Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/). Interesting points:\n",
    "    * Email.\n",
    "    * Github.\n",
    "    * Google Drive.\n",
    "    * HugggingFace dataset.\n",
    "    * Images.\n",
    "    * Jupyter Notebook.\n",
    "    * MS Excel.\n",
    "    * MS Powerpoint.\n",
    "    * MS Word.\n",
    "    * MongoDB.\n",
    "    * Pandas DataFrame.\n",
    "    * Twitter.\n",
    "    * URL.\n",
    "    * Unstructured file.\n",
    "    * WebBaseLoader.\n",
    "    * YouTube audio.\n",
    "    * YouTube transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1445736f-a89e-4df5-b354-274d7f80a1dd",
   "metadata": {},
   "source": [
    "## Step #2 of the Indexing Phase: Split data into small chunks\n",
    "* We’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks.\n",
    "* We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "* We set add_start_index=True so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute “start_index”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4772eaa0-7ab0-4a53-b553-a778b185713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eab95082-437d-4253-9cd1-0c10e2154bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d608bd-3e34-464b-a6ef-e6018b290ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57b40001-bf31-4de1-a727-288befe24c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 7056}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443726f5-1c91-4ce6-9f97-f73c64aede94",
   "metadata": {},
   "source": [
    "#### Additional info on Splitting chunks of text\n",
    "* [Documentation on Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/). Interesting points:\n",
    "    * Recursively split by character.\n",
    "    * Semantic chunking.\n",
    "    * Recursively split JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d580c-faa1-47bb-8272-86d9f6dc3097",
   "metadata": {},
   "source": [
    "## Step #3 of the Indexing Phase: Transform chunks into embeddings and store them in the vector database\n",
    "* Now we will index our 66 text chunks so that we can search over them.\n",
    "* When we want to search over the chunks, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored chunks with the most similar embeddings to our query embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "667b243c-58d6-4e1c-8c0e-83374b8118af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8a4830-94c1-4a3c-9d52-a04fd86a7569",
   "metadata": {},
   "source": [
    "#### Additional info on Embeddings\n",
    "* [Documentation on Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding).\n",
    "* [Integration with 3rd Party Embedding Models](https://python.langchain.com/docs/integrations/text_embedding/). Interesting points:\n",
    "    *  OpenAI.\n",
    "    *  Cohere.\n",
    "    *  Fireworks.\n",
    "    *  Google Vertex AI.\n",
    "    *  Hugging Face.\n",
    "    *  Llama-ccp.\n",
    "    *  MistralAI.\n",
    "    *  Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e321b-7c48-4af6-9fa8-e8aa62bdd5a3",
   "metadata": {},
   "source": [
    "#### Additional info on Vector Databases (also called Vector Stores)\n",
    "* [Documentation on Vector Databases](https://python.langchain.com/docs/modules/data_connection/vectorstores/).\n",
    "* [Integration with 3rd Party Vector Databases](https://python.langchain.com/docs/integrations/vectorstores/). Interesting points:\n",
    "    * Chroma.\n",
    "    * Faiss.\n",
    "    * Postgres.\n",
    "    * PGVector.\n",
    "    * Databricks.\n",
    "    * Pinecone.\n",
    "    * Redis.\n",
    "    * Supabase.\n",
    "    * LanceDB.\n",
    "    * MongoDB.\n",
    "    * Neo4j.\n",
    "    * Activeloop DeepLake. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdc1b8-1984-4340-a82f-f6331786f903",
   "metadata": {},
   "source": [
    "# Phase 2: Retrieval\n",
    "* We are going to create a RAG app that\n",
    "    * takes a user question,\n",
    "    * searches for documents relevant to that question,\n",
    "    * passes the retrieved documents and initial question to a model,\n",
    "    * and returns an answer.\n",
    "* First we need to define our logic for searching over documents. LangChain defines a Retriever interface which wraps an index that can return relevant Documents given a string query.\n",
    "* The most common type of Retriever is the VectorStoreRetriever, which uses the similarity search capabilities of a vector store to facilitate retrieval. Any VectorStore can easily be turned into a Retriever with VectorStore.as_retriever()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ab325-a4e7-4b45-9eb5-8737c14cff8d",
   "metadata": {},
   "source": [
    "#### Set the retriever\n",
    "A \"retriever\" refers to a component or object that is used to retrieve (or search for) specific information from a database or a collection of data. The retriever is designed to work with a vector database that stores data in the form of embeddings.\n",
    "\n",
    "Embeddings are numerical representations of data (in this case, text data split into smaller chunks) that capture the semantic meaning of the original content. These embeddings are created using models like the ones provided by OpenAI (for example, through `OpenAIEmbeddings`), which convert textual content into high-dimensional vectors that numerically represent the semantic content of the text.\n",
    "\n",
    "The vector database, in this context represented by `Chroma`, is a specialized database that stores these embeddings. The primary function of this database is to enable efficient similarity searches. That is, given a query in the form of an embedding, the database can quickly find and return the most similar embeddings (and by extension, the corresponding chunks of text or documents) stored within it.\n",
    "\n",
    "When the code sets the `retriever` as `vectorstore.as_retriever()`, it essentially initializes a retriever object with the capability to perform these similarity-based searches within the `Chroma` vector database. This means that the retriever can be used to find the most relevant pieces of information (text chunks, in this scenario) based on a query. This is particularly useful in applications that needs to retrieve information based on semantic similarity rather than exact keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11e8a468-4969-41b9-9b4d-cc367288a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bbb1fb2-3841-4eb0-8823-5b313a0cbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48c000-eec7-4b18-a848-eabcc4cffdba",
   "metadata": {},
   "source": [
    "* When we execute the previous cell, we start seeing our project on LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cb27b98-f8f5-4154-a6c2-6d0aa2390746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3e57393-1900-4ace-ba0d-b19b7902a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe940f5-db41-41fc-958c-0a1a1ccc85f9",
   "metadata": {},
   "source": [
    "#### Additional information on Retrievers.\n",
    "* [Documentation on Retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers/). Interesting points:\n",
    "    * Vectorstore.\n",
    "    * Time-Weighted Vectorstore: most recent first.\n",
    "    * Advanced Retrievers. \n",
    "* [Integrations with 3rd Party Retrieval Services](https://python.langchain.com/docs/integrations/retrievers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa17fd3a-955e-4e33-a8bd-da6404b7b179",
   "metadata": {},
   "source": [
    "# Phase 3: Generation\n",
    "* We will put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.\n",
    "* We’ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain LLM or ChatModel could be substituted in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3344a-dfbb-477c-9832-f55e0de94a05",
   "metadata": {},
   "source": [
    "#### Determine which Foundation LLM we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00328164-b63f-464b-bc2f-f5b16857f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea383f3a-c5aa-4c7b-9bb6-f6c4a94f09b2",
   "metadata": {},
   "source": [
    "#### Set the prompt\n",
    "* In this case, we will use a [pre-defined prompt from the LangSmith Prompt Hub](https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=ec6a7494-139b-5170-8044-143bc78622a9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d0f5d98-187b-4a64-a6bc-00891b74d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdffb63e-2384-4299-be87-386807e640c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To print the prompt we have selected from the LangSmith Prompt Hub\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed6565-9b09-47f5-b09a-9bf263409883",
   "metadata": {},
   "source": [
    "* Alternative with a custom prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36aa0397-fe94-42fe-a9c5-88b25a3a9eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# template = \"\"\"Answer the question based only on the following context:\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb9b01cd-f3f0-47ba-b5e1-512d9a3e3ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \n",
    "     \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bef3ea-2954-4170-a8fe-17d70480293a",
   "metadata": {},
   "source": [
    "* You can see the previous operation reflected on LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6333d2d-7bc3-4ede-8811-619e76e6dc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c48285-cc04-4d5d-ab22-052390a5c424",
   "metadata": {},
   "source": [
    "#### Define a pre-processing function to better format the document\n",
    "* This step is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6af2a1-d21d-4d89-ac61-ad9c5e743d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2f877-c83f-469b-947d-7e38253a2c0e",
   "metadata": {},
   "source": [
    "#### Define the RAG chain\n",
    "Pay attention to this details:\n",
    "* See how we use the retriever as context\n",
    "* See how we use the pre-processing function\n",
    "* See how we use the StrOutputParser to format the answer as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "551d3282-3ebc-4ac0-ae6d-a46396601a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97636524-8600-4f99-90a2-660c6c82cb72",
   "metadata": {},
   "source": [
    "* Using the LCEL Runnable protocol to define the chain allows us to:\n",
    "    * pipe together components and functions in a transparent way.\n",
    "    * automatically trace our chain in LangSmith.\n",
    "    * get streaming, async, and batched calling out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea3e40-91f8-470f-a646-b1a22e0bc8fb",
   "metadata": {},
   "source": [
    "### Runnables\n",
    "A \"Runnable\" is like a tool or a small machine in a factory line that has a specific job. You can think of it as a mini-program that knows how to do one particular task, like read data, change it, or make decisions based on that data.\n",
    "\n",
    "A \"Runnable\" in the context of LangChain is essentially a component or a piece of code that can be executed or \"run\". \n",
    "\n",
    "It acts like a building block that can perform a specific operation or process data in a particular way.\n",
    "\n",
    "The chaining or composition of Runnables allows you to build a sequence where data flows from one Runnable to another, each performing its function, ultimately leading to a final result.\n",
    "\n",
    "In essence, a Runnable makes it easy to define and reuse modular operations that can be combined in various ways to build complex data processing and interaction pipelines efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410800a-cc2c-4881-b32f-8c86a2c05e2d",
   "metadata": {},
   "source": [
    "#### RunnableParallel\n",
    "RunnableParallel is like a manager that can handle multiple tasks at once in a software system. Instead of doing one job at a time, it can do several jobs simultaneously, making the process faster and more efficient.\n",
    "\n",
    "RunnableParallel is used to manage different components that do tasks like retrieving information, passing questions, or formatting data, and then brings together all their outputs neatly. This makes it very handy for building complex operations where multiple, independent processes need to run at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d35588-2476-4a21-be66-06e670b3f34e",
   "metadata": {},
   "source": [
    "#### RunnablePassThrough\n",
    "RunnablePassthrough is a simple tool that lets data pass through it without making any changes. You can think of it like a clear tube: whatever you send into one end comes out the other end unchanged.\n",
    "\n",
    "When working with data transformations and pipelines, RunnablePassthrough is often used alongside other tools that might modify the data. For example, when multiple operations are run in parallel, you might want to let some data flow through unaltered while other data gets modified or processed differently.\n",
    "\n",
    "RunnablePassthrough is often used in combination with RunnableParallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1578b327-3ee1-4974-bfe3-3dfac4ffa48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# runnable = RunnableParallel(\n",
    "#     passed=RunnablePassthrough(),\n",
    "#     modified=lambda x: x[\"num\"] + 1,\n",
    "# )\n",
    "\n",
    "# runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a66ce8-c42e-48a6-bc27-b831fe1b2cf8",
   "metadata": {},
   "source": [
    "##### Output:\n",
    "{'passed': {'num': 1}, 'modified': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836650d-b362-4904-a5bb-788e1146826a",
   "metadata": {},
   "source": [
    "**In the previous example:** \n",
    "   - `RunnableParallel` is set up with two keys, `passed` and `modified`.\n",
    "   - Under the `passed` key, `RunnablePassthrough` is used. This means that the data under this key remains unchanged. In the example, it passes the input `{'num': 1}` directly to the output.\n",
    "   - Under the `modified` key, a function modifies the data by adding 1 to the value of `num`, so the output becomes 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181664d-0bb0-4e8a-bfbe-48a167f4997a",
   "metadata": {},
   "source": [
    "**In our example:**\n",
    "   - Here, `RunnablePassthrough` helps in a retrieval scenario where it is used to directly pass the user's question to a model that needs it to generate a response.\n",
    "   - While `RunnablePassthrough` takes care of the question, another component fetches relevant context data to be used with the question for generating the response.\n",
    "\n",
    "In summary, RunnablePassthrough is useful when you want to ensure some parts of the input data are transmitted through a system or process without any changes, while other parts might be actively modified or enriched.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f519a2c-4976-4270-9080-c1d7048db1c6",
   "metadata": {},
   "source": [
    "#### We can start asking questions to our RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941ecb5-5cc7-4cff-85e8-50662ce78794",
   "metadata": {},
   "source": [
    "* We will use streaming, so the answer will flow like what we see using chatGPT instead printing the whole answer in one single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94e080f6-499a-47ab-bf9e-9794f770f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a technique that breaks down complex tasks into smaller, more manageable steps. It involves transforming big tasks into multiple simpler tasks to aid in the interpretation of the model's thinking process. Task decomposition can be done using prompting techniques, task-specific instructions, or human inputs."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650d4fa-63b7-44b9-9dfd-8bfc88f9123f",
   "metadata": {},
   "source": [
    "* Now you can see the previous operation reflected on LangSmith."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed9690-a91d-4130-8427-bc12b6d65638",
   "metadata": {},
   "source": [
    "#### Alternative approach with a customized prompt and without streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b904a59-75b8-4cf6-a4d0-3b1dbd211ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down complex tasks into smaller and simpler steps to make them more manageable for autonomous agents or AI systems. This approach allows for better planning and execution of tasks by focusing on individual components rather than tackling the entire task at once. Thanks for asking!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d0afdd-492b-4575-9e37-1c328ce6668a",
   "metadata": {},
   "source": [
    "* You can see the previous operation reflected on LangSmith."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c7305b-5f4d-44ff-bef1-44a53e74138d",
   "metadata": {},
   "source": [
    "#### Additional Information on Chat Models and LLM Models\n",
    "* [Documentation on Chat Models](https://python.langchain.com/docs/modules/model_io/chat/). Interesting points:\n",
    "    * How-To Guides.\n",
    "    * Function calling.\n",
    "    * Streaming. \n",
    "* [Integration with 3rd Party Chat Models](https://python.langchain.com/docs/integrations/chat/). Interesting points:\n",
    "    * OpenAI.\n",
    "    * Anthropic.\n",
    "    * Azure OpenAI.\n",
    "    * Cohere.\n",
    "    * Fireworks.\n",
    "    * Google AI.\n",
    "    * Hugging Face.\n",
    "    * Llama 2 Chat.\n",
    "    * MistralAI.\n",
    "    * Ollama.\n",
    "* [Documentation on LLM Models](https://python.langchain.com/docs/modules/model_io/llms). Interesting points:\n",
    "    * How-To Guides.\n",
    "    * Streaming.\n",
    "    * Tracking token usage. \n",
    "* [Integrations with 3rd Party LLM Models](https://python.langchain.com/docs/integrations/llms). Interesting points:\n",
    "    * OpenAI.\n",
    "    * Anthropic.\n",
    "    * Azure OpenAI.\n",
    "    * Cohere.\n",
    "    * Fireworks.\n",
    "    * Google AI.\n",
    "    * GPT4All.\n",
    "    * Hugging Face.\n",
    "    * Llama.cpp\n",
    "    * Ollama.\n",
    "    * Replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413171e-f533-4897-9335-f42a197a1bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
